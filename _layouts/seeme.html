---
layout: page
---
<div id="seeme">
    <section class="bg"></section>
    <h1 class="title">{{ page.title }}</h1>
    <div class="wrapper">
    </div>

    <div align="center">
        <h1>Social EgoMesh Estimation</h1>
        <h2><i>Winter Conference on Applications of Computer Vision (WACV) 2025</i></h2>
        <h5 style="color:grey"> <i>Luca Scofano, Alessio Sampieri, Edoardo De Matteis, Indro Spinelli, Fabio Galasso
</i>
        </h5>
        <!-- <h5 style="color:grey"> <i>Sapienza University of Rome, Italy</i></h5> -->
    </div>

    <hr>
    <p align="center"><button type="button" class="btn btn-primary"><a href="#abstract">Abstract</a></button>
        <!-- <button type="button" class="btn btn-primary"><a href="#qualitative">Paper Presentation</a></button> -->
        <button type="button" class="btn btn-primary"><a href="https://arxiv.org/pdf/2411.04598">Paper</a></button>
        <!-- <button type="button" class="btn btn-primary"><a href="#poster-and-slides">Poster and Slides</a></button> -->
        <!-- <button type="button" class="btn btn-primary"><a href="#talk">Talk</a></button> -->
        <button type="button" class="btn btn-primary"><a
                href="https://github.com/L-Scofano/SEEME">Code</a></button>
    
                <hr>

            </p>
    <p align="center">
        <img src="\assets\img\projects\seeme\teaser.jpg" width="700" height="600" alt="centered image" />
    </p>
    

    <hr>
    <h1 id="abstract">Abstract</h1>
    Accurately estimating the 3D pose of the camera wearer in egocentric video sequences is crucial to modeling human behavior in virtual and augmented reality applications. The task presents unique challenges due to the limited visibility of the user's body caused by the front-facing camera mounted on their head. Recent research has explored the utilization of the scene and ego-motion but it has overlooked humans' interactive nature. We propose a novel framework for Social Egocentric Estimation of body MEshes (SEE-ME). Our approach is the first to estimate the wearer's mesh using only a latent probabilistic diffusion model which we condition on the scene and for the first time on the social wearer-interactee interactions. Our in depth study sheds light on when social interaction matters most for ego-mesh estimation: it quantifies the impact of interpersonal distance and gaze direction. Overall SEE-ME surpasses the current best technique reducing the pose estimation error (MPJPE) by 53%
</div>